{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the JRC Excel files\n",
    "\n",
    "### JRC Data\n",
    "\n",
    "ExpDam is direct expected damage per year from river flooding in Euro (2010 values). Data includes baseline values (average 1976-2005) and impact at SWLs.\n",
    "\n",
    "All figures are multi-model averages based on EC-EARTH r1 to r7 (7 models)\n",
    "\n",
    "PopAff is population affected per year from river flooding. Data includes baseline values (average 1976-2005) and impact at SWLs.\n",
    "\n",
    "All figures are multi-model averages based on EC-EARTH r1 to r7 (7 models)\n",
    "\n",
    "Reference\n",
    "Alfieri, L., Bisselink, B., Dottori, F., Naumann, G., de Roo, A., Salamon, P., Wyser, K. and Feyen, L.: Global projections of river flood risk in a warmer world, Earths Future, doi:10.1002/2016EF000485, 2017.\n",
    "\n",
    "### Note:\n",
    "\n",
    "We need to calculate anomalies against the historical base period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from iso3166 import countries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_netcdf_and_csv_files(path='data/'):\n",
    "    \"\"\"Crawl through a specified folder and return a dict of the netcdf d['nc']\n",
    "    and csv d['csv'] files contained within.\n",
    "    Returns something like\n",
    "    {'nc':'data/CNRS_data/cSoil/orchidee-giss-ecearth.SWL_15.eco.cSoil.nc'}\n",
    "    \"\"\"\n",
    "    netcdf_files = []\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if isinstance([], type(files)):\n",
    "            for f in files:\n",
    "                if f.split('.')[-1] in ['nc']:\n",
    "                    netcdf_files.append(''.join([root,'/',f]))\n",
    "                elif  f.split('.')[-1] in ['csv']:\n",
    "                    csv_files.append(''.join([root,'/',f]))\n",
    "    return {'nc':netcdf_files,'csv':csv_files}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_value(df, swl, verbose =False):\n",
    "    \"\"\"Extract the historical and absolute SWL values and calculate\n",
    "    an anomaly.\n",
    "    \"\"\"\n",
    "    if verbose: print(df[swl].values)\n",
    "    if 'PopAff_1976-2005' in data_slice:\n",
    "        historical_key = 'PopAff_1976-2005'\n",
    "        #print(\"In pop aff\")\n",
    "    elif 'ExpDam_1976-2005' in data_slice:\n",
    "        historical_key = 'ExpDam_1976-2005'\n",
    "    else:\n",
    "        raise ValueError('Found no historical data in the file')\n",
    "    # Get the SWL mean\n",
    "    try:\n",
    "        tmp_abs = float(''.join(df[swl].values[0].split(\",\")))\n",
    "    except:\n",
    "        tmp_abs = None\n",
    "    # Get the historical mean\n",
    "    try:\n",
    "        tmp_historical = float(''.join(df[historical_key].values[0].split(\",\")))\n",
    "        if tmp_historical == 0: tmp_historical = None\n",
    "    except:\n",
    "        tmp_historical = None\n",
    "    #print(tmp_historical, tmp_abs)\n",
    "    if all([tmp_historical, tmp_abs]):\n",
    "        anomaly = tmp_abs - tmp_historical\n",
    "    else:\n",
    "        anomaly = None\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "def gen_output_fname(fnm, swl_label):\n",
    "    path = '/'.join(fnm.split('/')[1:3])\n",
    "    file_name = swl_label+'_'+fnm.split('/')[-1]\n",
    "    tmp_out = '/'.join(['./processed/admin0/', path, file_name])\n",
    "    return tmp_out\n",
    "\n",
    "gen_output_fname(fnm, swl)\n",
    "\n",
    "\n",
    "def process_JRC_excel(fnm):\n",
    "    # I should loop over the set of shapes in gadams8 shapefile and look for the country in the data...\n",
    "    # SIMPLIFIED SHAPES FOR ADMIN 0 LEVEL\n",
    "    s = gpd.read_file(\"./data/gadm28_adm0_simplified/gadm28_adm0_simplified.shp\")\n",
    "    raw_data = pd.read_csv(fnm)\n",
    "    # Note 184 are how many valid admin 0 areas we got with the netcdf data.\n",
    "    keys =['name_0','iso','variable','swl_info',\n",
    "        'count', 'max','min','mean','std','impact_tag','institution',\n",
    "        'model_long_name','model_short_name','model_taxonomy',\n",
    "        'is_multi_model_summary','is_seasonal','season','is_monthly',\n",
    "        'month']\n",
    "    swl_dic = {'SWL1.5':1.0, 'SWL2':2.0, 'SWL4':4.0}\n",
    "\n",
    "    possible_vars = {'data/JRC_data/river_floods/PopAff_SWLs_Country.csv':'river_floods_PopAff',\n",
    "                          'data/JRC_data/river_floods/ExpDam_SWLs_Country.csv':'river_floods_ExpDam'}\n",
    "\n",
    "    num_swls = 0\n",
    "    for swl in ['SWL1.5','SWL2', 'SWL4']:\n",
    "        num_swls += 1\n",
    "        tot = 0\n",
    "        valid = 0\n",
    "        extracted_values = []\n",
    "        meta_level1 = {'variable':possible_vars[fnm],\n",
    "                       'swl_info':swl_dic[swl],\n",
    "                       'is_multi_model_summary':True,\n",
    "                       'model_short_name':'EC-EARTH',\n",
    "                       'model_long_name': \"Projections of average changes in river flood risk per country at SWLs, obtained with the JRC impact model based on EC-EARTH r1-r7 climate projections.\",\n",
    "                       'model_taxonomy': 'EC-EARTH',\n",
    "                       'is_seasonal': False,\n",
    "                       'season': None,\n",
    "                       'is_monthly':False,\n",
    "                       'month': None,\n",
    "                       'impact_tag': 'w',\n",
    "                       'institution':None,\n",
    "                        }\n",
    "        for i in s.index:\n",
    "            tot += 1\n",
    "            meta_level2 = {'name_0': s.name_0[i],\n",
    "                           'iso': s.iso[i],}\n",
    "            tmp_mask = raw_data['ISO3_countryname'] == meta_level2['iso']\n",
    "            data_slice = raw_data[tmp_mask]\n",
    "            if len(data_slice) == 1:\n",
    "                #print(meta_level2['iso'])\n",
    "                extracted = extract_value(data_slice, swl)\n",
    "                dic_level3 = {'min':None,\n",
    "                              'mean': extracted,\n",
    "                              'max': None,\n",
    "                              'count':None,\n",
    "                              'std':None}\n",
    "                valid += 1\n",
    "                # FIND ALL VALUES NEEDED BY KEY\n",
    "                # WRITE TO EXTRACTED_VALUES\n",
    "                d = {**meta_level1, **meta_level2, **dic_level3}\n",
    "                extracted_values.append([d[key] for key in keys])\n",
    "        tmp_df = pd.DataFrame(extracted_values, columns=keys)\n",
    "        output_filename = gen_output_fname(fnm, swl)\n",
    "        path_check ='/'.join(output_filename.split('/')[:-1])\n",
    "        # WRITE EXTRACTED VALUES TO A SPECIFIC SWL CSV FILE IN PROCESSED\n",
    "        if not os.path.exists(path_check):\n",
    "            os.makedirs(path_check)\n",
    "        tmp_df.to_csv(output_filename, index=False)\n",
    "        print('Created ', output_filename)\n",
    "    print('TOTAL in loop:', tot)\n",
    "    print('valid:', valid)\n",
    "    print(\"Looped for\", num_swls, 'swls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created  ./processed/admin0//JRC_data/river_floods/SWL1.5_PopAff_SWLs_Country.csv\n",
      "Created  ./processed/admin0//JRC_data/river_floods/SWL2_PopAff_SWLs_Country.csv\n",
      "Created  ./processed/admin0//JRC_data/river_floods/SWL4_PopAff_SWLs_Country.csv\n",
      "TOTAL in loop: 240\n",
      "valid: 184\n",
      "Looped for 3 swls\n",
      "Created  ./processed/admin0//JRC_data/river_floods/SWL1.5_ExpDam_SWLs_Country.csv\n",
      "Created  ./processed/admin0//JRC_data/river_floods/SWL2_ExpDam_SWLs_Country.csv\n",
      "Created  ./processed/admin0//JRC_data/river_floods/SWL4_ExpDam_SWLs_Country.csv\n",
      "TOTAL in loop: 240\n",
      "valid: 184\n",
      "Looped for 3 swls\n"
     ]
    }
   ],
   "source": [
    "fs = identify_netcdf_and_csv_files(path='data/JRC_data')\n",
    "fs['csv']\n",
    "for fnm in fs['csv']:\n",
    "    process_JRC_excel(fnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
