{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using helix_funcs\n",
    "\n",
    "An example of how to use the helix_funcs module to process the HelixScope datafiles, and generate a master output table.\n",
    "\n",
    "Process_files should generate a similarly named csv file in the process folder.\n",
    "combine_processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helix_funcs\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s = gpd.read_file('./data/gadm28_adm1/gadm28_adm1.shp')  # OLD\n",
    "#s = gpd.read_file(\"./data/gadm28_adm1_simplified/gadm28_adm1_simplified.shp\")   # <--- ADMIN LEVEL 1\n",
    "#s = gpd.read_file('./data/gadm28_countries/gadm28_countries.shp')  # <--- ADMIN LEVEL 0\n",
    "#s = gpd.read_file(\"./data/sanitized_grid/sanitized_grid.shp\")  #<-- 10x10 grid\n",
    "s = gpd.read_file(\"./data/good_five_grid/good_five_grid.shp\")\n",
    "#s = s.to_crs(epsg='4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = helix_funcs.identify_netcdf_and_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs['nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UEA_data/climate/tx/HADGEM3-R1.SWL_15.cl.tx.JJA.nc is seasonal data. Skipping To process set skip_seasonal to False\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for f in ['data/UEA_data/climate/tx/HADGEM3-R1.SWL_15.cl.tx.JJA.nc']:\n",
    "    helix_funcs.process_file(file=f, shps=s, shape_id='grids5', overwrite=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "Dont process the monthly data, but note that the seasonal data e.g. MAM need to be checked in the parsing, as the variables are not being properly selected.\n",
    "\n",
    "Also check that all the metadata is being parased when the new country level shapes are uses.\n",
    "\n",
    "Create 10x10 degree grid to cover the globe. Intersect it with land areas. Create an ID for each shape. Run the pipeline against those shapes to create a coarse gridded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./processed/grids/UEA_data/biodiversity/orchidee-giss-ecearth1.SWL_2.bd.mammalnobiodiversity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 'data/UEA_data/climate/tx/HADGEM3-R3.SWL_4.cl.tx.MAM.nc'\n",
    "print(f)\n",
    "extract_medata_from_filename(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = [\"MAM\", \"JJA\", \"SON\", \"DJF\"]\n",
    "\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed up \n",
    "\n",
    "Looks like we achieve a 34 second execution per file (down from 2 mins) when we use a simplifed set of geometries.\n",
    "\n",
    "\n",
    "\n",
    "## Investiage areas with no zonal stats\n",
    "\n",
    "Seems like small areas are failing. Need to investigate best way to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* check simple shapes produce same number of elements as complex shapes - confimred\n",
    "* check shapefile has same number of shapes as output file - confirmed\n",
    "* check simple and complex processed results for same file show good agreement. - confirmed\n",
    "\n",
    "* Add kwarg for ADMIN0 or Admin1. Have a differnt root folder for both in processed data.\n",
    "* MOVE alredy processed ADMIN1 level data to the `processed/admin1/` directory - Failed. Columns changed. Will need to recalculate :(\n",
    "* SIMPLIFY the ADMIN-0 level geometry file as I did with the ADMIN1 file. - confirmed\n",
    "* RUN ADMIN0 for all data.\n",
    "\n",
    "Perhaps we should have two tables: ADMIN0 level (based on simplified shapes), and ADMIN1 level.\n",
    "\n",
    "The ADMIN 0 level makes sense to calculate (mean, min, max, stdev), as those shapes should be large. BUT for ADMIN 1 level, the shapes can be far smaller. In which case it doesnt make sense to calculate those stats for every shape.\n",
    "\n",
    "* given time, we could work out which shapes it makes sense for (e.g. with counts > 3) and make some distinction.\n",
    "* with short time (which is the case) it maybe only makes sense to offer mean over those areas (which in most cases will be the mean of 1 cell (i.e. simply the value of the gridcell).\n",
    "\n",
    "\n",
    "THEREFORE:\n",
    "\n",
    "1. Process all data to ADMIN 0 (using simplified country shapes)\n",
    "    * use this as MVP\n",
    "2. CALCULATE TIME RELATIVE STATISTICS\n",
    "    * i.e. for datatets with Month distinctions calculate the seasonality\n",
    "3. Attempt to do the ADMIN 1 level data.\n",
    "\n",
    "# Deal with outputs\n",
    "\n",
    "Reduce significant digits. Join together dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = helix_funcs.identify_netcdf_and_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(fs['csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "helix_funcs.combine_processed_results(path='./processed/grids',\n",
    "                                      table_name='./master_10x10.csv')  #<-- join all results files together into a master_admin1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('./master_admin0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(tmp['variable'].unique()),'\\n')\n",
    "for var in (tmp['variable'].unique()):\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(tmp['impact_tag'].unique()),'\\n')\n",
    "for var in (tmp['impact_tag'].unique()):\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = tmp['variable'] == 'river_floods_PopAff' #'river_floods_PopAff' #\"river_floods_ExpDam\"\n",
    "shorter = tmp[mask]\n",
    "shorter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(tmp['swl_info'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(tmp['iso'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_season = tmp['is_seasonal'] == True\n",
    "print((len(tmp[is_season])/len(tmp)) * 100.,\"% has seasonal data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp[tmp['is_seasonal']]['variable'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp[tmp['is_seasonal'] == True]['season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 5\n",
    "if a == 1 or a == 5:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_season = tmp['is_monthly'] == True\n",
    "print((len(tmp[is_season])/len(tmp)) * 100.,\"% has monthly data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_files = helix_funcs.identify_netcdf_and_csv_files('processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(output_files['csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(outputs['csv'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_mask = tmp['iso'] == 'ESP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = tmp[country_mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admin_mask = tmp['id_1'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp['id_1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test plots\n",
    "\n",
    "We need to make an easy way to preview how a specific variables choropleths will appear.\n",
    "This means read a file, and make a preview plot using geopandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_files = helix_funcs.identify_netcdf_and_csv_files('processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helix_funcs.map_file_by_iso(f=output_files['csv'][10], s=s, iso='FRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLIFY LIFE!\n",
    "\n",
    "Before passing the polygons for zonal analysis, they should be simplified. \n",
    "Looks like the call should be `stest.geometry.simplify(0.2, preserve_topology=False)`\n",
    "\n",
    "The geometry files themsevels should be simplified, and read in already done. I have another notebook to simply (and correct) these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stest.geometry.simplify(0.2, preserve_topology=False).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stest = s[s['iso'] == 'BRA']\n",
    "stest.geometry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stest.geometry.simplify(0.2, preserve_topology=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Work out problem of small shapes (probably will need to have a logical test for small shapes, and buffer the geom before zonal stats are calculated). Perhaps this should even be done prior to any loop, initially when the shapes are calculaed.\n",
    "\n",
    "First can look and see what admins are absent from file (to see where the problem lies).\n",
    "\n",
    "Test with simplifed shapes.\n",
    "\n",
    "#### REDUCE SIZE OF FINAL DATA BY removing significant digits from values\n",
    "e.g. 10.6466969914 should be converted to simply 10.6\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_processed_results(path='./processed/admin1',\n",
    "                              table_name=\"./master_admin1.csv\"):\n",
    "    \"\"\"Combine all the csv files in the path (e.g. all processed files)\n",
    "    into a single master table\n",
    "    \"\"\"\n",
    "    output_files = identify_netcdf_and_csv_files(path)\n",
    "    frames = [pd.read_csv(csv_file) for csv_file in output_files['csv']]\n",
    "    master_table = pd.concat(frames)\n",
    "    master_table.to_csv(table_name, index=False)\n",
    "    print(\"Made {0}: {1:,g} rows of data. {2:,g} sources.\".format(table_name,\n",
    "                                                        len(master_table),\n",
    "                                                        len(output_files['csv'])\n",
    "                                                                 ))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='./processed'\n",
    "output_files = helix_funcs.identify_netcdf_and_csv_files(path)\n",
    "frames = [pd.read_csv(csv_file) for csv_file in output_files['csv'][0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for  n, frame in enumerate(frames):\n",
    "    print(n, min(frames[0]['min']), max(frames[0]['min']), min(frames[0]['max']), max(frames[0]['max']), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = frames[0].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admin_level = 0\n",
    "\n",
    "if admin_level == 0:\n",
    "    admin_prefix = 'admin0/'\n",
    "elif admin_level == 1:\n",
    "    admin_prefix = 'admin1/'\n",
    "else:\n",
    "    raise ValueError(\"admin_level kwarg must be either 0 or 1\")\n",
    "print(admin_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test new land shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gpd.read_file('./data/sanitized_grid/sanitized_grid.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.geometry[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = test.drop(['featurecla', 'scalerank','id'],axis=1)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gs = tmp.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tmp_geoms = gs.simplify(0.2, preserve_topology=False)  # removes all little areas (no point calculating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp['index'] = list(range(len(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp.to_file(\"/Users/Ben/Desktop/my_land_grid/my_land_grid.shp\", driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xstr = tmp.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('/Users/Ben/Desktop/my_grd_land.json','w') as f:\n",
    "#    f.write(xstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
