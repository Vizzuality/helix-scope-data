{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELIX-SCOPE\n",
    "## Country Impacts Summaries\n",
    "### Part 1\n",
    "\n",
    "In this notebook we will produce national summary statics from the climatic modelling outputs provided by the Helix consortium. Summary statistics (mean, max, min and standard deviation) will be calculated for every country and variable, and where possible, for every model and model run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import re\n",
    "import fiona\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterstats import zonal_stats\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS\n",
    "First let's define a few functions to separate the analysis tasks as we iterate through each country polygon/impact layer:\n",
    "\n",
    "- __ncAttributes:__  this function extracts metadata from gridded climate files. It does so by using a combination of `netCDF4` functions and REGEX commands. The results are returned as a dictionary.\n",
    "\n",
    "- __zstats:__ This is basically a customisation of the `rasterstats.zonal_stats` function. It’s used to extract summary statistics of interest, returning these values as a dictionary.\n",
    "\n",
    "- __climateSummaries:__ Integrates the 2 functions above, returning  a dictionary consisting of the concatenation of the above functions\n",
    "\n",
    "- __emptyDict:__ Returns an empty dictionary where the summary statistics and attribute information will be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ncAttributes(filepath):\n",
    "    \"\"\"\n",
    "    Purpose: To extract useful metadata from nc files, such as variable names, model_taxonomies, SWL and so on.\n",
    "    Process: Most of this information is stored in the files’ global attribute description, we will access them using the netCDF4.ncattrs function. A couple of attributes are also encoded in the file name themselves, these will be extracted using REGEX commands.\n",
    "    Input:  A string with the file’s path, starting from the data folder\n",
    "    Output: This will return a dictionary object with: SWL_info, impact_tag, institution, is_multi_model_summary, is_seasonal, model_long_name, model_short_name, model_taxonomy, variable\n",
    "    \"\"\"\n",
    "\n",
    "    # --- extract .nc global attribute data\n",
    "    #: model_long_name, is_seasonal, is_multi_model_summary, SWL_info, impact_tag, institution\n",
    "    nc_file = Dataset(filepath, 'r')\n",
    "    nc_globalatt_dic = {}\n",
    "    nc_attrs = nc_file.ncattrs()\n",
    "    for nc_attr in nc_attrs:\n",
    "        nc_globalatt_dic.update({nc_attr: nc_file.getncattr(nc_attr)})\n",
    "    \n",
    "    # convert text to bools where relevant\n",
    "    nc_globalatt_dic['is_multi_model_summary'] = nc_globalatt_dic['is_multi_model_summary'] in ['true', 'True', 'TRUE']\n",
    "    nc_globalatt_dic['is_seasonal'] = nc_globalatt_dic['is_seasonal'] in ['true', 'True', 'TRUE']\n",
    "\n",
    "    # --- extract additonal data from filename\n",
    "    fname = filepath.split(\"/\")[4]\n",
    "    variable = filepath.split(\"/\")[3]\n",
    "    model_taxonomy = re.search('(^.*?)\\.',fname, re.IGNORECASE).group(1)\n",
    "    model_short_name = re.search('(^.*?)-',model_taxonomy, re.IGNORECASE).group(1)\n",
    "    \n",
    "    # -- create attribute dictionary\n",
    "    del nc_globalatt_dic['contact']    \n",
    "    nc_att_dic = {\"model_short_name\" : model_short_name, \"variable\" : variable, \"model_taxonomy\" : model_taxonomy}\n",
    "    nc_att_dic.update(nc_globalatt_dic)\n",
    "\n",
    "    return nc_att_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zstats(country_shp, rast, zstats_vars,nc_file):\n",
    "    \"\"\"\n",
    "    Purpose: To extract country summaries in a standardised format\n",
    "    Process: The bulk of this function uses the zonal_stats stats function to perform the calculations. \n",
    "    We’re just adding a few pre-defined objects to execute this function in a for loop, \n",
    "    as we iterate through each country polygon and climate data file.\n",
    "    Inputs:  country_shp = a shapefile , rast = a rasterio.read object, zstats_vars = a predefined list of variables ,nc_file = a rasterio.open object\n",
    "    Output: This will return a dictionary object with: max, mean, min, std\n",
    "    \"\"\"\n",
    "\n",
    "    # get stats\n",
    "    rast_zstats = zonal_stats(country_shp, rast[0], \n",
    "                      stats= zstats_vars,\n",
    "                      all_touched=True,\n",
    "                      raster_out=False,\n",
    "                      affine=nc_file.profile['transform'], \n",
    "                      nodata= nc_file.profile.get(\"nodata\"))\n",
    "    \n",
    "    # encode in dictionary format\n",
    "    stats_dic = {\"mean\" : rast_zstats[0]['mean'], \n",
    "                 \"max\" : rast_zstats[0]['max'],\n",
    "                 \"min\" : rast_zstats[0]['min'],\n",
    "                 \"std\": rast_zstats[0]['std'],\n",
    "                 \"count\": rast_zstats[0]['count']}\n",
    "    \n",
    "    return stats_dic\n",
    "\n",
    "\n",
    "# define zonal sats of interest to be uused with the abive function\n",
    "zstats_vars = ['mean', 'max','min','std','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def climateSummaries(filepath,country_shp,zstats_vars,nc_file,rast):\n",
    "    \"\"\"\n",
    "    Purpose: To integrate attribute and summary statistics data extraction within each iteration \n",
    "    Inputs:  country_shp = a shapefile , rast = a rasterio.read object, zstats_vars = a predefined list\n",
    "    of variables ,nc_file = a rasterio.open object\n",
    "    Output: This will return a dictionary object that concatenates  the dictionary objects resulting from the ncAttributes and zstats funcions\n",
    "    \"\"\"\n",
    "\n",
    "    # get zonal stats\n",
    "    stats_dic = zstats(country_shp,rast,zstats_vars,nc_file)\n",
    "\n",
    "    # get nc file attributes\n",
    "    nc_att_dic = ncAttributes(filepath)\n",
    "\n",
    "    # -- Build 'data row' (for pandas) ---\n",
    "    row = stats_dic\n",
    "    row.update(nc_att_dic)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emptyDict():\n",
    "    \"\"\"\n",
    "    Used to generate an empty dictionary object, \n",
    "    which will be used to store the values resulting from the summary iterations\n",
    "    \"\"\"\n",
    "    return {\n",
    "    'country':[],\n",
    "    'impact_tag':[],\n",
    "    'variable':[],\n",
    "    'SWL_info':[],\n",
    "    'model_short_name':[],\n",
    "    'max':[],\n",
    "    'mean':[],\n",
    "    'min':[],\n",
    "    'std':[],\n",
    "    'count':[],\n",
    "    'model_long_name':[],\n",
    "    'is_seasonal':[],\n",
    "    'is_multi_model_summary':[],\n",
    "    'iso2':[],\n",
    "    'model_taxonomy':[],\n",
    "    'institution':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read data files\n",
    "\n",
    "Below we will:\n",
    "\n",
    "1. Create a list of all available impacts layers for processing (`filepaths` object)\n",
    "2. Load a global national boundaries shapefile (`countries_shp` object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get .nc filenames and paths\n",
    "rootdir = \"data/Helix/\"\n",
    "\n",
    "# create file paths list\n",
    "filepaths = []\n",
    "for (subdir, dirs, files) in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        fpath = os.path.join(subdir, file)\n",
    "        if re.search('.nc$',fpath):\n",
    "            filepaths.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read countries\n",
    "countries_shp = gpd.read_file('./data/minified_gadm28_countries/gadm28_countries.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Asynchronous Batch Processing\n",
    "The processing of impact layers will occur in discrete batches as more climatic data becomes available form the HELIX consortium. Since this is a time consuming process, we are keeping a record of the analysed layers in the file:  `processed/processed-ncdfs.txt`. Impact layers that have been processed will be removed from the `filepaths` object so they’re skipped when running the batch analysis script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read list of processed files\n",
    "processed = \"processed/processed-ncdfs.txt\"\n",
    "processed_list =[]\n",
    "\n",
    "with open(processed, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        processed_list.append(line)\n",
    "\n",
    "# Remove processed files from filepaths list     \n",
    "filepaths = list(set(filepaths) - set(processed_list))\n",
    "filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run Batch Processing\n",
    "\n",
    "For each country polygon and impact layer the process will be to:\n",
    "\n",
    "1. Extract the layers' metadata details\n",
    "2. Clip impact layers with the country polygons\n",
    "3. Extract the layers' summary statistics (mean, max, min, std) \n",
    "4. Concatenate all summary outputs in a single dataframe\n",
    "\n",
    "The resulting output will be stored as a csv file in the folder `processed/raw-summaries`\n",
    "\n",
    "The script takes about __1.5 hrs__ to generate summaries for 250 climatic layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define empty DF\n",
    "data = emptyDict()\n",
    "\n",
    "# ----  PRODUCE SUMMARIES ---\n",
    "for filepath in filepaths:\n",
    "    \n",
    "    # read ncdf\n",
    "    nc_file = rasterio.open(filepath)\n",
    "    rast = nc_file.read()\n",
    "    \n",
    "    for i in countries_shp.index.values:\n",
    "        \n",
    "        # load country shape\n",
    "        country_shp = countries_shp.iloc[[i]]\n",
    "        \n",
    "        # get country_shp attibutes\n",
    "        country_att_dic = {'iso2' : country_shp.iso2.to_string(index=False),\n",
    "                           'country' : country_shp.name_engli.to_string(index=False)}\n",
    "\n",
    "        row = climateSummaries(filepath,country_shp,zstats_vars,nc_file,rast)\n",
    "        row.update(country_att_dic)\n",
    "        \n",
    "        for var in row:\n",
    "            data[var].append(row[var])\n",
    "    \n",
    "    # append filepath to list of processed impact layers\n",
    "    with open(processed, 'a+') as file:\n",
    "        file.write(filepath)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---- MAKE DATAFRAME ---\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine data types and save `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWL_info                  float64\n",
      "count                       int64\n",
      "country                    object\n",
      "impact_tag                 object\n",
      "institution                object\n",
      "is_multi_model_summary       bool\n",
      "is_seasonal                  bool\n",
      "iso2                       object\n",
      "max                       float64\n",
      "mean                      float64\n",
      "min                       float64\n",
      "model_long_name            object\n",
      "model_short_name           object\n",
      "model_taxonomy             object\n",
      "std                       float64\n",
      "variable                   object\n",
      "dtype: object\n",
      "Shape:(59243, 16)\n"
     ]
    }
   ],
   "source": [
    "# examine dataframe \n",
    "print(df.dtypes)\n",
    "print('Shape:' + str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "tday = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "fname  = \"%s%s.csv\" % (\"processed/raw-summaries/joined-summaries-\", tday)\n",
    "df.to_csv(fname,index=False,na_rep='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
